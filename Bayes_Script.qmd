---
title: "Script Bayes Statistic"
author: "Mathias Nägele, Andreas Busjahn"
bibliography: Bayes_references.bib
format: 
  docx:
    #reference-doc: D:/1) Mathias/Internship/Stochastical Models/template_1.docx
    toc: true
    toc-depth: 3
execute:
  echo: false
  warning: false
  output: asis
fig.dpi: 150
fig-width: 6
fig-height: 4
editor: visual
#citations: 
#  style: numeric
---

## The Bayes Theorem

Bayesian statistics allows us to go from what is known – the data (the results of the coin throw here) – and extrapolate backwards to make probabilistic statements about the parameters (the underlying bias of the coin) of the processes that were responsible for its generation

The Bayesian statistics allows us to go from what is known – the data (e.g. the results of a coin throw) and go backwards to make probabilistic statements about the parameters (e.g. is it a fair coin that means is the parameter of the corresponding binomial distribution 0.5) of the processes that were responsible for its generation [@Lambert]. It is to come from effect (probability is the effect significant) back to cause, it is often called inverse probability (not the best term ever, because there is a connotation from pure mathematics, better was reverse or backwards) or Bayesian inference. This is generally speaking the inductive form to gain knowledge.

### History

We start with a general criticism of the inductive form for gaining knowledge, because this was a driving force for the development of the Bayes Theorem.\
David Hume (1711 – 1776, Scottish philosopher, economist and historian) argues that "causes and effects are discoverable, not by reason, but by experience". In other words, we can never be certain about the cause of a given effect. For example, we know from experience that if we push a glass off the side of a table, it will fall and shatter, but this does not prove that the push caused the glass to shatter. It is possible that both the push and the shattering are merely correlated events, reflecting some third, and hitherto unknown, ultimate cause of both 1). He formulate the well non dilemma of correlation (two effects show similarities but are caused by an unknown common cause) or true cause.

This criticism was extensively worked out in the philosophical work "Logik der Forschung (1934) of Karl Popper (Austrian-British philosopher) . He formulated Humes finding more precise and postulated that a theory/cause can only be falsified never verified, as it is applied in the Null-Hypothesis test (there it is the question if the Null-Hypothesis can be falsified). But it does not make a postulated cause more possible. The falsification merely says the no-effect can be discarded, hence there is an effect, because the corresponding probability to find such an effect level under the distribution given as the Null-Hypothesis is low (e.g. 5%). The effect might be statistical proven, but the cause is still not clarified. The cause is often "proven" by exposing/observing the system under investigation by different levels of the postulated cause and is then investigated by ANOVA, regression or the other tools of machine learning.

Anyway, common sense and obvious facts out of experience should not be ignored by reason of pure philosophy. Of course, the false clause from merely correlation to be declared as a cause is a danger and should be taken into account. Learning is very often an inductive process from special effects summarized at the end in a common cause. Here the Bayesian Theorem can help to trace back an effect to a cause. Furthermore from a theoretical point of view the Bayes theorem is derived out of the basic axioms of the calculus of probability, there is no approximation or other fancy assumptions, it is pure mathematics and a genius insight. What is made out of it, as always in statistics, is another story.

Thomas Bayes ( 1701 – 1761 English statistician, philosopher and Presbyterian )

Bayes thought about how to apply mathematics, specifically probability theory, to the study of cause and effect (the motivation for him to think about it, is not exactly known up to date, the response to the criticism of Hume cam later in). It is about to get a better information out of a previous (a priori) results, experience etc. At the end he was able to formulate the law of *conditional probability*. The experiment (not a very exited one, but instructive) he made to come to his law is described in *Link oder referenzieren* .

Ít was presented to the Rocal Sciety, but it seems that Bayes himself was perhaps not keen about it, and never published his work, Bayes's solution to a problem of [inverse probability](https://en.wikipedia.org/wiki/Inverse_probability "Inverse probability") was presented by his friend Richard Price (1723 -1791, Welsh moral philosopher, Nonconformist minister) to the [Royal Society](https://en.wikipedia.org/wiki/Royal_Society "Royal Society")  in [*An Essay Towards Solving a Problem in the Doctrine of Chances*](https://en.wikipedia.org/wiki/An_Essay_Towards_Solving_a_Problem_in_the_Doctrine_of_Chances "An Essay Towards Solving a Problem in the Doctrine of Chances"), which was read in 1763 after Bayes's death when he worked on it for two years– correcting some mistakes and adding references – and eventually sent it to the Royal Society. His intention was to have a way to counter Hume’s attack on causation (see above) and was aslos motivated by religious questions. Hume’s argument was unsettling to Christianity because God was traditionally known as the First Cause of everything. The mere fact that the world exists was seen as evidence of a divine creator that caused it to come into existence . Hume’s argument meant that we can never deal with absolute causes; rather, we must make do with probable causes (perhaps it all came out of the Big Bang, but what was the cause of the Big Bang, a spontaneous disproportionation of energy given matter and anit-matter?). This weakened the link between a divine creator and the world that we witness and, hence, undermined a core belief of Christianity. *Link oder referenzieren*.

Independently of Bayes, Pierre-Simon Laplace (1749 – 182,7 France mathematician, physicist, astronomer, philosopher) had already begun to work on a probabilistic way to go from effect back to cause (inverse probability), and in 1774 he get notice by the work of Bayes and Price and gave him confidence to pursue his ideas. Laplace still needed an example application of his method that was easy enough for him to calculate, yet interesting enough to garner attention. His sample comprised the numbers of males and females born in Paris from 1745 to 1770. This data was easy to work with because the outcome was binary – the child was recorded as being born a boy or girl – and was large enough to be able to draw conclusions from it. In the sample, a total of 241945 girls and 251527 boys were born hence a total of 493472 births were considered. Laplace used this sample and his theory of inverse probability to estimate that there was a probability of approximately 10 ^−42^ that the sex ratio favored girls rather than boys. On the basis of this tiny probability, he concluded that he was as ‘certain as any other moral truth’ that boys were born more frequently than girls. This was the first practical application of Bayesian inference as we know it now. Laplace went from an effect – the data in the birth records – to determine a probable cause – the ratio of male to female births is not 50:50 on the bases of the data). Later in his life, Laplace also wrote down the first modern version of Bayes’ mathematical rule that is used today, where causes could be given different prior probabilities. Hence it would be more accurate to call the Bayesian inference theory as the Bayes–Price– Laplace rule.

### Theory of Bayesian theorem explained by Laplace\` data on birth rate

The “easy” example chosen by Laplace can be used to get the basic idea of Bayesian statistic. It is also a good learning about calculating with the concept of (conditional) probability which is the basics of Bayesian statistics and that may had been the intention of Laplace.  

But let us first clarify some vocabulary of probability theory with the births example (or if you like it more flipping a coin which can simulate birth rate of 0.5 if the coin is fair). The language and the laws are based on the set theory.

An elementary event $\omega$ is a possible result of a random "experiment", in the example of births it is "birth of a girl" or "birth of a boy" or. The event space birth$\Omega = \{ \text{Girl}, \text{Boy} \}$ is the set containing all possible results of the random experiment.

Now we can define partial sets A of the event space $\Omega$ in our case a partial case can be $A= \{ \text{Girl}\}$ or $B= \{ \text{Boy}\}$ . At this point the experiment flipping a coin is a little bit unfortunate because in the case of binary data "A" correponds to the elemtary event $\omega$ too. If there was more then two elementary events the definition of partial sets makes more sense e.g a die with six different colored sides: $\Omega= \{ \text{blue, red, yellow, white, black, brown}\}$ then a subset is $A= \{ \text{blue, yellow}\}$ .

Now its getting picky: A Random variable X is the result of a function which assigns to each object or aspect of an object a figure. Please be aware, at this point, it is not a number with a value but a label hence a categorical number. It makes it easier in mathematics and in programming. The R-object of a factor is doing that:

```{r, echo=TRUE}


#The character vector colors of a die is mutated to a factor. The character vector contains the elementary events of the event space. In the object factor they get a categorial number:
color_die <- factor(c("blue", "red", "yellow", "white", "black", "brown"))


(event_space <- levels(color_die))
(lable_numbers <- labels(color_die))
(elementary_event <- levels(color_die)[2])
(event_subset <- levels(color_die)[1:2])

#Please recognize, the random_numbers are displayed in quotation marks identifying them as real labels not values

```

Let us define the sub set $A = \{ \text{blue}, \text{red}\}$ Now we can express them with the random variable:

$A = \{ \text{X=1 or X=2}\}$ or shorter $A = \{ \text{X>3}\}$ the last expression shows you the advantage of using catogorial numbers instead of the name of elemental events .

Finally we came to the central and very intuitive term of probability. Laplace defined the probability to be

$P(A |\Omega)\ =\ \frac{number\ of\ elementary\ events\ in\ A}{total\ number\ of\ elementary\ events\ \Omega}$

The term $P(A|\Omega)$ is expressed as "the probability of A given B" .It is often just given as $P(A)$ if $\Omega$ is an event space and not only a partial set because then it is $P(\Omega) =1$. The prerequisite for this definition of probability is that all elementary events have the same probability and have a finite number of elements (if not we speak about probability density but that´s a topic for its on and we will skip it).

More simple and catchy is to say

$P(A |\Omega)\ =\ \frac{number\ of\ favorable\ events\ in\ A}{number\ of\ possible\ events\ \Omega}$

\
Is e.g. the measurement (relization of a random event) of the mass of an object (apple, peach etc. ) an elementary even in the sense of Laplace? No measurements of the mass will chose certain events more likely than others, hence the prerequisite of same probability is not fulfilled. Furthermore the value space has not a finite number, it is a continuum. In this case we cannot even speak from a probability of a value, but it is a probability density, which has an infinitesimal low value. Only the integration over a certain range of probability density will return a value of a probability.

In the case of a coin or a dice we can see the true value after toss the object. But in case of measurements like the mass on an object, we have to speak about a "true value". But this true value is a concept and is not observable. Only in the case of the primordial mass in Paris which had getting a arbitrary mass of 1 kg the "true value", is a labeled value and was known at the time it was defined. All derived objects are compared to the mass and will have some more or less minimal deviations. Here is one of the principal difference between the frequentic view on probability and the Bayesian approach. The first says there is a "true value" and the more measurment values you get the more you will reach the "true value", hence the true value is not a random variable but a fixed value. Each convidence interval (CI) is constructed around the so called excpected value (often mean value) which is the random variable and not around the true value. The CI is constructed on the with the same distribution as the H0 Hypothesis. Given the P-value \< 0.05%, this means, if you take several samples from a population and calculate a CI for each of these samples, about 95% of these intervals would include the true value of the entire population. But if the measurement is highly accurate (small CI) but there is a bias in this the measurement, hence it is incorrect at a certain degree, the CI might even not contain the true value in extreme cases. On the other hand the Bayesian statistics do not handle the concept of a true value of a paramter but probability distribution around an a priori value of the parameter and furtheron there might be more information then you get another distribution around an a posterior value with the Bayes statistic.

### Elementary events and normal distribution

Please create with an R script a loop generating 10 elementary events with a random number generator of floating numbers between the interval \[0,1\] (as per defintion the elementary events should have all the same probability). Calculate the mean value and do it 10, 100 and 1000 times ... in a loop. Display the results in a histogram.What can you say about the shape of the distribution. Test the distribution of the mean values for normality.

```{r, echo=TRUE}
library(ggplot2)
library(tidyverse)

count_uniform <- 10 #number of the random numbers 
num_means <- 1000  # Number of mean values to be calculated  
mean_values <- c(num_means)  # Initializing the vector of mean values

for(i in 1:num_means) {
  random_numbers <- runif(count_uniform, min = 0, max = 1)
  mean_values[i] <- mean(random_numbers)
}

mean_values_tibble<-tibble(mean_value_col = mean_values)

#ggplot needs the datastructure of a dataframe or a tibble, in aes the column names of that are used
ggplot(data = mean_values_tibble, mapping = aes(x=mean_value_col)) + 
  geom_histogram(bins=30)


kstest_mean_values <- ks.test(x = mean_values,
        y="pnorm",
     mean=mean(mean_values),
       sd=sd(mean_values),
      exact=F)


(kstest_mean_values)


```

What we can see is that for means calculated out of sets with (even low) numbers of elementary events (which have per definition the same probability and are therefore uniformly distributed) are approaching a normal distribution. Out of data having only the information about the interval where the random data can be found, we get structured information with a much higher "content" of information in form of the normal distribution. This demonstrated why normal distribution is so central in statistics. Furthermore each measurement can be seen as a combination of many elementary events.

We have seen the principal notations and the definition of probability we go now further steps to the conditional probability and finally to the Bayes (Price, Laplace) Theorem or "inverse probability" and what is all about. Those vocabulary can be shown in a tree diagram. We use other birth data to make it more clear (pure binary birth data are not so interesting at this point, because no conditional probability can be drawn out).

## Twin data

The propability that twins ar monozygotic is about 1/4 (in Europe) and of course they have all the time the same sex. Given we have the elementary event $\omega$ of twins and we define the possible results of the elementary event as Monozygotic twins = M Dizygotic twins = D

The event space $\Omega = (M, D)$

Hence the probabilities for the two possible outcomes given there are twins is $P(M | \Omega)=1/4, P(D |\Omega)=3/4$ So that's the wording and symbol for a conditional probability. Now we look on the second event that is, twins have the identical sex ($I$) or not ($I^C$) (the C stands for complementary, hence for not having the same sex). Given the twins are monozygotic, the probability is $P(I | M)=1$ and $P(I^C| M)=0$ they are both either female or male, for the dizygotical twins it is: $P(I | D)=P(I^C| D)=1/2$ hence both the probability both hav e the same sex or not is equal and 5is 0%.

Let as look on the situation with a probability tree:

![Tree_digramm of monozygotic and dizygcotic twins](Tree_diagram.png)

There are only a view rules to be recoginzed we know from the set theory :\
1. Probabilities on a branches are multiplied\
2. The sum of all probabilities of the different branches has to be 1.\
3. The commutative law of intersect of sets says the sequence of does not matter e.g. it is :

$D\ \cap\ I\ =\ I\ \cap\ D$

And the same is true for probabilities, we have only to go backwards on the branch (if the condition given the event space :

$P(M)*P\left(I|M\right)=P\left(M|I\right)*P(I)$

Now we can rearrange this formula and get the Bayesian Theorem:

$P\left(M|I\right)=\frac{P(M)*P\left(I|M\right)}{P(I)}$

Hence e.g. we can answer the question what is the probability for monozygcotic twins given the babies have the same sex, if we know the terms on the right site of the formula. This is because it is often called inverse probability. So we got the Bayesian Theorem, that is it.

The only term we do not know yet is $P(I)$. This information we can get out of the tree diagram too. It is in principle a new smaller event space which lead to higher probabilities for the elementary events in this space. The $P(I)$ is the probability of the smaller event space $I =$ "Twins with the same sex"in the event space $\Omega$ of all Twins. The space $I$ is identified with the grey frame:

![Tree_digramm of monozygotic and dizygcotic twins with identical sex](Tree_frame_diagram.png)

Now we can summarize over the branches containing the state I:

$P(I)=\ P(M)*P\left(I|M\right)+P(D)*P\left(I|D\right)$

Remark: The condition "given the event space $\Omega$" is often omitted in the notation per convention.

The formula looks quite strait forward, but the term $P(I)$ called marginal Likelihood especially is usually a monster as the name itself (the term Likelihood in this context make less sense and we should not much care about it, what it can be looked at is given above). To summarize over all possible state of the new "event space" in more complex investigations is not easy and especially in the case of continuous data integration has to be used instead of summation and numerical Monte Carlo (see below) simulation has often to be applied. That was perhaps the reason Bayes Method was not used until the introduction of the computer.

## Exercise

You have following situation. Two dark boxes containing 10 balls each. In the box A there are 7 red balls and 3 white balls. In the box B there is 1 red ball and 9 white balls. Now the lady lack take out of one box a red ball. What is the probability this ball went out of the box A (inverse probability)?

1.  Please draw the probability tree and set up the equation to get the inverse probability, and the marginal likelihood:

![Tree_digramm of red and white balls in two boxes](Balls_in_boxes.png)

$P(A)*P\left(R|A\right)=P\left(A|R\right)*P(R)$

The inverse probability the red ball went out of box A is $P\left(A|R\right)=\frac{P(A)*P\left(R|A\right)}{P(R)}$

2.  Calculate the marginal likelihood

The marginal likelihood is given as $P(R)=\ P(A)*P\left(R|A\right)+P(B)*P\left(R|A\right)$

3.  What is the marginal Likelihood in simple words? This is nothing else then the probability to take a red ball out all the boxes. It is the probability of the subset red balls in the elementary space "balls in the boxes", there are 8 red balls within 20 balls. To draw a red ball has the probability of 8/20= 2/5. For this reason $P(R)$ is:

$P(R)=\frac{1}{2}*\frac{7}{10}+\frac{1}{2}*\frac{1}{10}=\frac{8}{20}$

*Unfortunately it is called as a Likelihood, but it is not, in fact the marginal Likelihood is a probability! Likelihood looks how good a parameter can explain the data, it is not a probability but a way to find the maximum likelihood for the data given a distribution. In the case of Gaussian distribution e.g. the maximum likelihood is the mean value. This is where confusion starts, with imprecise wording.*

4.  Eventually get the probability we looking for.\
    $P(A|R)=\frac{\frac{7}{20}}{\frac{7}{20}+\frac{1}{20}}=\frac{7}{8}$
5.  How the information: It is a red ball" change the probability in contrast to the experiment "drawing a red ball out of the boxes?

The elementary space is reduced to the elementary space 8 red balls. Given a red ball is drawn 7 of the eight balls are in box A.\
What is the probability to draw a red ball out of the box A = 1/2\*7/10 = 7/20. This probability is much lower than that one if you know a red ball is taken so where it is coming from. Here the intuition may end.

-\> Apply on a clinical data like what is the probability the test is positive the patient is ill or decision making as in differential diagnoses

Up to now we have examples where we do not speak about probability distribution. The probabilities were obvious by counting or given by information Now we go to examples were the probability has to be calculated out o a distribution because counting is not possible or very cumbersome.

## SARS-Cov-2 testing examined with Bayes´theorem

The article of [@Bentley] discuss the error rates of test equipment for SARS-Cov-2 (Cov) under field conditions and those obtained in the laboratory. We will take some aspects which are given in the article to see how the Bayes theorem is used in a scientific investigation.

in the following there is the results of RT-PCR hospital-administrated for more then 1000 patients. The RT-PCR test is seen as the "gold standard" for the determination of Cov:

```{r}

library(tidyverse)

Confusion_tibble<- tibble(c("Postive", "Negative"), 
                           c(308, 580), 
                           c(105, 21))




```

## Laplace´ data of births in Paris

Let us come back to the Laplace experiment with the birth rate of girls and boys in Paris from 1745 to 1770? The elementary events with the results "yes" or no "are" binomial distributed, and can be applied for the birth rates. What Laplace wanted like to show was the possibility for a birth rate of 0.5 when the number of the girls born in the observed period is 241945 with total birth of 493472.

We start with the probability of finding $k_G$ female births with a birth rate of (probability of the elementary event of a birth) 0.5. The possible number of births are binomial distributed hence we get for the probability $P(k_G|p,t)$ following:

$P(k_G|\ p,\ t)=\left(\begin{matrix}t\\k_G\\\end{matrix}\right)p^{k_G}{(1-p)}^{t-k_G}$

To get the formula of the posteriori probability we have to change the parameter p and $k_G$ to get the probability in question:

$P(p|\ k_G,\ t)=\frac{P(k_G|\ p,\ t)P(p)}{P(k_G, \ t)}$

Where:

$P(p|\ k_G,\ t)$ : The posterior probability of p given the data $k_G$ and total trials t.

$P(k_G|\ p,\ t)$: The probability of k births of girls given the parameter p

$P(p)$: The probability (density) of p. With Laplace´ a priori assumption in the discrete case we handle P(p=0.5)=1

$P(k_G|\ t)$: This is marginal likelihood.

We have given following information:

1.  $P(k_G|p,t)$ given via the formula of the binomila distribution. With the binomial coefficient

$\left(\begin{matrix}t\\k_G\\\end{matrix}\right)=\frac{t!}{k_G!(t-k_G)!}$

If we introduce the numerical value we can see that the numbers to be calculated are much to small as to be handled by hand or the computer

$P(k_G=241945|\ p=0.5,\ t=493472)=\left(\begin{matrix}493472\\241945\\\end{matrix}\right){0.5}^{241945}{(1-0.5)}^{493472-241945}$

What we can do is to build the logarithm, all number are getting much smaller and multiplication is now changed to addition and the power operation to a multiplication:

$ln\left[P(k_G|\ p,\ t)\right]=ln\left(\begin{matrix}t\\k_G\\\end{matrix}\right)+(k_G)ln(p)+(t-k_G)ln(1-p)$

Can be obtained by the very good Stirling approximations for high values of :

$ln\left(\begin{matrix}t\\k_G\\\end{matrix}\right)\approx t*ln(t)-k_Gln(k_G)-(t-k_G)ln(t-k)-\frac{(1-p)}{2}ln(2\pi t p)$

2.  The a priori probability is the parameter $P(p=0.5)$. Laplace set the probability to 1 . That´s the "knowledge" before he may get a new value out of his investigation. $P(p=0.5)$ = 1
3.  And what is the value $P(k_G| \ t)$ ? This is often the elephant in the room and the promised monster:

$P(k_G| t)=\int_{0}^{1}{P(k_G|p,t)*P(p)dp}$

It is the integration (you can call it a summation for the continuous variable p) over all possible values of the elementary probability P(p) for the parameter p weighted by its probability ... . But we should remember it is the probability of the subset of the count of births $k_G$ for each parameter p between the interval \[0,1\] in the whole elementary space (all $k_G$ in the interval \[0, 493472\]?)

In the most cases this integral cannot be determined or it is very cumbersome. But each integral can be seen as summation over infinite increments. This is the basic of each approximation of an integral and that´s how a Monte Carlo process it is working. E.g. we

$P(p|k_G,t)\approx\frac{1}{m}\sum_{i=0}^{m}{P(k|p_i, t)}P(p_i)$

with the increment $p_i=\frac{i}{m}$

In the case of m = 10 we get $p_i$ =0.1, 0.2, ....0.5,... 0.9, and we assume for all the same probability, hence each it has the probability $P( p_i )$=1.

Furthermore we have again the binomial distribution

$P(k_G|\ p_i,\ t)=\left(\begin{matrix}t\\k_G\\\end{matrix}\right)p_i^{k_G}{(1-p_i)}^{t-k_G}$

We get finally the approximation:

$P(p|k_G,t)\approx\frac{1}{m}\sum_{i=0}^{m}\left(\begin{matrix}t\\k_G\\\end{matrix}\right)p_i^{k_G}{(1-p_i)}^{t-k_G}$

The binomial coefficient is the same as given above. The $p_i$ can be run over different values and summed up.

So this is quite easy to compute. It can be done with different values of i to look on the distribution of that approximation. One thing has to be taken into account. The term

## R-Code for $P(k_G|p,t)$

```{r}
fbirths_vect <- c(0.51, 241945, 493472) #parameter vector (fbirths_vect) of the female births 

ln_bino_coef <- fbirths_vect[3]*log(fbirths_vect[3])-
                fbirths_vect[2]*log(fbirths_vect[2])-
              (fbirths_vect[3]-fbirths_vect[2])*log(fbirths_vect[3]-fbirths_vect[2])-
               log(2*pi*fbirths_vect[3]*fbirths_vect[1]*(1-fbirths_vect[1]))/2

ln_coef <- fbirths_vect[2]*log(fbirths_vect[1])+
            (fbirths_vect[3]-fbirths_vect[2])*log(1-fbirths_vect[1])

ln_bino_dist <- ln_bino_coef+ln_coef


bino_dist<-exp(ln_bino_dist)


cat("The probability of k births of girls given the parameter p",fbirths_vect[1],":", bino_dist, "\n")

```

## R-Code for $P(k_G|t)$

according to the approximation in 3.

```{r}
# values for fbirths_vect[2] and fbirths_vect[3]
fbirths_vect_2 <- fbirths_vect[2]
fbirths_vect_3 <- fbirths_vect[3]

# range for fbirths_vect[1]
start_value <- 0.488    # start value
end_value <- 0.509      # end value
step_size <- 0.001      # Ssteps

# Generation of vector fbirths_vect[1]
fbirths_vect_1 <- seq(from = start_value, to = end_value, by = step_size)

# Empty list for the results
results <- list()

# Varaible with start value of the summed results
total_bino_dist <- 0

# Loop über die Werte von fbirths_vect[1]
for (value in fbirths_vect_1) {
  ln_bino_coef <- fbirths_vect_3 * log(fbirths_vect_3) -
                  fbirths_vect_2 * log(fbirths_vect_2) -
                  (fbirths_vect_3 - fbirths_vect_2) * log(fbirths_vect_3 - fbirths_vect_2) -
                  log(2 * pi * fbirths_vect_3 * value * (1 - value)) / 2

  ln_coef <- fbirths_vect_2 * log(value) +
             (fbirths_vect_3 - fbirths_vect_2) * log(1 - value)
  
  ln_bino_dist <- ln_bino_coef + ln_coef
  
  # save the results in list
  results[[as.character(value)]] <- exp(ln_bino_dist)
  
  # Sum up the values in the list
  total_bino_dist <- total_bino_dist + exp(ln_bino_dist)
}

# Display the results
print(results)


cat("The approximation of the marginal likelihood:", total_bino_dist, "\n")

```

## R-code for the final result of the a posteriori probability

for the parameter p=0.5 given $k_G=241945$

```{r}

cat("a posteriori probability for the parameter",fbirths_vect[1],":",bino_dist/total_bino_dist )
```

But this is not still end of story. He only know the paramter 0.5 given the data is very very unlikely. But he knew $P(p<0.5)\approx\ 1-P(p>0.5)$ (because $P(p=0.5 \approx\  0)$, as we have senn above and all higher probabilities of p get even smaller he conclude $P(p<0.5)\approx\ 1$. This is what Laplace get as the result. He published it 1820 and he was as "certain as any other moral truth" that boys were born more frequently than girls.

Excersice

Please use the Z-test to look if the Nullypothese p=0.5 can be rejected

# Literatur
