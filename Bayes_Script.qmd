---
title: "Script Bayes Statistic"
author: "Mathias Nägele, Andreas Busjahn"
format: 
  docx:
    reference-doc: D:/1) Mathias/Internship/Stochastical Models/template_1.docx
    toc: true
    toc-depth: 3
execute:
  echo: false
  warning: false
  output: asis
fig.dpi: 150
fig-width: 6
fig-height: 4
editor: visual
##link-citations: true
---

## The Bayes Theorem

General speaking: it is about to came from effect back to cause, this is the inductive form to gain knowledge by the calculus of probability, it is often called inverse probability (not the best term ever)

### History

We start with a general criticism of the inductive form for gaining knowledge, because this was a driving force for the discovery of the Bayes Theorem.\
David Hume (1711 – 1776, Scottish philosopher, economist and historian) argues that ‘causes and effects are discoverable, not by reason, but by experience’. In other words, we can never be certain about the cause of a given effect. For example, we know from experience that if we push a glass off the side of a table, it will fall and shatter, but this does not prove that the push caused the glass to shatter. It is possible that both the push and the shattering are merely correlated events, reflecting some third, and hitherto unknown, ultimate cause of both 1). He formulate the well non dilemma of correlation (two effects show similarities but are caused by an unknown common cause) or true cause.

This criticism was extensively worked out in the phylosophical work "Logik der Forschung (1934) of Karl Popper (Austrian-British philosopher) . He formulated Humes finding more precise and postulated that a theory/cause can only be falsified never verified, as it is applied in the Null-Hypothesis test which is tried to be falsified. But it does not make a postulated cause more possible, the falsification merely says the no-effect can be discarded if it is proved that to observe the effect based on the Null-hypothesis is very unlikely, hence the effect and cause should be clearly distinguished. The effect might be statistical proven, but what is the cause is not clarified. The cause is often "proven" by exposing/observing the system under investigation by different levels of the postulated cause and is then investigated by ANOVA, regression or the other tools of machine learning.

Anyway, common sense and obvious facts out of experience should not be ignored by reason of pure philosophy. Of course, the false clause from merely correlation to be declared as a cause is a danger and should be taken into account. Learning is very often an inductive process from special effects summarized at the end in a common cause. Here the Bayesian Theorem can help to trace back an effect to a cause. Furthermore from a theoretical point of view the Bayes theorem is derived out of the basic axioms of the calculus of probability, there is no approximation or other fancy assumptions, it is pure mathematics and a genius insight. What is made out of this is as always in statistics, another story.

Thomas Bayes ( 1701 – 1761 English statistician, philosopher and Presbyterian minister)

Bayes thought about how to apply mathematics, specifically probability theory, to the study of cause and effect (the motivation for him to think about it, is not exactly known up to date). It is about to get a better information out of a previous (a priori) results, experience etc. At the end he was able to formulate the law of *conditional probability*. The experiment (not a very exited one) he made to come to his law is described in *Link oder referenzieren* .

Ít was presented to the Rocal Sciety, but it seems that Bayes himself was perhaps not keen about it, and never published his work, Bayes's solution to a problem of [inverse probability](https://en.wikipedia.org/wiki/Inverse_probability "Inverse probability") was presented by his friend Richard Price to the [Royal Society](https://en.wikipedia.org/wiki/Royal_Society "Royal Society")  in [*An Essay Towards Solving a Problem in the Doctrine of Chances*](https://en.wikipedia.org/wiki/An_Essay_Towards_Solving_a_Problem_in_the_Doctrine_of_Chances "An Essay Towards Solving a Problem in the Doctrine of Chances"), which was read in 1763 after Bayes's death when he worked on it for two years– correcting some mistakes and adding references – and eventually sent it to the Royal Society. His intention was to have a way to counter Hume’s attack on causation (see above) and was at least partially motivated by religious question. Hume’s argument was unsettling to Christianity because God was traditionally known as the First Cause of everything. The mere fact that the world exists was seen as evidence of a divine creator that caused it to come into existence . Hume’s argument meant that we can never deal with absolute causes; rather, we must make do with probable causes (perhaps it all came out of the Big Bang, but what was the cause of the Big Bang, a spontaneous disporoportioning of energy?). This weakened the link between a divine creator and the world that we witness and, hence, undermined a core belief of Christianity. *Link oder referenzieren*.

Independently of Bayes, Pierre-Simon Laplace (1749 – 1827 mathematician, physicist, astronomer philosopher) had already begun to work on a probabilistic way to go from effect back to cause (inverse probability), and in 1774 he get notice by the work of Bayes and Price and gave him confidence to pursue his ideas. Laplace still needed an example application of his method that was easy enough for him to calculate, yet interesting enough to garner attention. His sample comprised the numbers of males and females born in Paris from 1745 to 1770. This data was easy to work with because the outcome was binary – the child was recorded as being born a boy or girl – and was large enough to be able to draw conclusions from it. In the sample, a total of 241945 girls and 251527 boys were born hence a total of 493472 births were considered. Laplace used this sample and his theory of inverse probability to estimate that there was a probability of approximately 10 ^−42^ that the sex ratio favored girls rather than boys. On the basis of this tiny probability, he concluded that he was as ‘certain as any other moral truth’ that boys were born more frequently than girls (OK, I´m not totally convinced if he really stated it like that, because an equal birth rate seems to me not excluded like that, or?). This was the first practical application of Bayesian inference as we know it now. Laplace went from an effect – the data in the birth records – to determine a probable cause – the ratio of male to female births. Later in his life, Laplace also wrote down the first modern version of Bayes’ mathematical rule that is used today, where causes could be given different prior probabilities. Hence it would be more accurate to call the Bayesian inference theory as the Bayes–Price– Laplace rule.

### Theory of Bayesian theorem explained by Laplace\` data on birth rate

The “easy” example chosen by Laplace can be used to get the basic idea of Bayesian statistic. It is also a good learning about calculating with the concept of (conditional) probability which is the basics of Bayesian statistics and that may had been the intention of Laplace.  

Let us first clarify some vocabulary of probability theory with the births example (or if you like it more flipping a coin which can simulate birth rate of 0.5 if the coin is fair). The language and the laws are based on the set theory.

An elementary event $\omega$ is a possible result of a random "experiment", in the example of births it is "Girl" or "Boy" or. The event space $\Omega = \{ \text{Girl}, \text{Boy} \}$ is the set containing all possible results of the random experiment.

Now we can define partial sets A of the event space $\Omega$ in our case a partial case can be $A= \{ \text{Girl}\}$ or $B= \{ \text{Boy}\}$ . At this point the experiment flipping a coin is a little bit unfortunate because in the case of binary data "A" correponds to the elemtary event $\omega$ too. If there was more then two elementary events the definition of partial sets makes more sense e.g $\Omega= \{ \text{blue, red, yellow}\}$ then a subset is $A= \{ \text{blue, yellow}\}$ .

Now its getting picky: A Random variable X is the result of a function which assigns to each object or aspect of an object a figure. Please be aware it is not a number with a value but a label hence a categorical number. It makes it easier in mathematics and in programming. The R-object of a factor is doing that:

```{r, echo=TRUE}


#The character vector colors of a die is mutated to a factor. The character vector contains the elementary events of the event space. In the object factor they get a categorial number:
color_die <- factor(c("blue", "red", "yellow", "white", "black", "brown"))


(event_space <- levels(color_die))
(lable_numbers <- labels(color_die))
(elementary_event <- levels(color_die)[2])
(event_subset <- levels(color_die)[1:2])

#Please recognize, the random_numbers are displayed in quotation marks identifying them as real labels not values

```

Let us define the sub set $A = \{ \text{blue}, \text{yellow}\}$ Now we can express them with the random variable:

$A = \{ \text{X=1 or X=2}\}$ or$A = \{ \text{X>3}\}$ the last expression shows you the advantage of using random variables instead of the possible elemental events.

Finally we came to the central and very intuitive term of probability. Laplace defined the probability to be

$P(A |\Omega)\ =\ \frac{number\ of\ elementary\ events\ in\ A}{total\ number\ of\ elementary\ events\ \Omega}$

The term $P(A|\Omega)$ is expressed as "the probability of A given B" .It is often just given as $P(A)$ if $\Omega$ is an event space and not only a partial set.\
A prerequisite is that all elementary events have the same probability and the events are independent from each other and have a finit number elements.

More simple and catchy is to say

$P(A |\Omega)\ =\ \frac{number\ of\ favorable\ events\ in\ A}{number\ of\ possible\ events\ \Omega}$

Exercise: Please apply the same terms and concept to a measurement of e.g mass of objects:\
- Is the mass of an object (apple, peach etc. ) an elementary even in the sense of Laplace? No measurements of the mass will chose certain events more likely than others, hence the prerequisite of same probability is not fullfilled

-   Is a value of a measurement the realization of a random variable? /= yes, 10.011 kg is an an event of the random variable X. X comprises all possible values e.g 10.011 kg which might be observed after a measurement.\
-   Out of which space the random values esteem ? /= it is the space of all real numbers equal or bigger then zero - Why the "true value" can be seen as a label of the object? /= it is a concept and is not observable. Only in the case of the primordail mass in Paris which had getting a arbitrary mass of 1 kg the "true value", labled value was known at the time it was defined. - But what an intuitive approximation of the true value might tell you? /= The a priori knowledge about the order of magnitude a random variables can have, hence the probability of the realization of a random variable So we are not primarely looking on the value of a random variable but its probability to occur.

Later on we will see how the realization of physicalas a random varialbe can be modeled by out of elementary events.

Please create with an R script a loop generating 10 elementary events with a random number generator of floating numbers between the interval \[0,1\]. The elementary events should have all the same probability. Calculate the mean value and do it 10, 100 and 1000 times in a loop. Display the results in a histogram.What can you say about the shape of the distribution. Test the mean values for normality for the data of the mean values.

```{r, echo=TRUE}
library(ggplot2)
library(tidyverse)

count_uniform <- 10 #number of the random numbers 
num_means <- 1000  # Number of mean values to be calculated  
mean_values <- c(num_means)  # Initializing the vector of mean values

for(i in 1:num_means) {
  random_numbers <- runif(count_uniform, min = 0, max = 1)
  mean_values[i] <- mean(random_numbers)
}

mean_values_tibble<-tibble(mean_value_col = mean_values)

#ggplot needs the datastructure of a dataframe or a tibble, in aes the column names of that are used
ggplot(data = mean_values_tibble, mapping = aes(x=mean_value_col)) + 
  geom_histogram(bins=30)


kstest_mean_values <- ks.test(x = mean_values,
        y="pnorm",
     mean=mean(mean_values),
       sd=sd(mean_values),
      exact=F)


(kstest_mean_values)


```

Hence, what we can see is that means calculated out of sets with even low number of elementary events (which have per definition the same probability and are therefore uniformly distributed) are approaching a normal distributed. Out of data having only the information about the interval where the random data can be found, we get structured information with a much higher information of the normal distribution. This is a demonstration why normal distribution is so central in statistics. Furthermore each measurement can be seen as a combination of elementary events.

We have seen the principal notations and the definition of probability we go now further steps to the conditional probability and finally to the Bayes (Price, Laplace) Theorem or "inverse probability" and what is all about. Those vocabulary can be shown in a tree diagram. We use other birth data to make it more clear (pure binary birth data are not so interesting at this point, because no conditional probability can be drawn out).

## The twin data

The propability that twins ar monozygotic is about 1/4 (in Europe) and of course they have all the time the same sex. Given we have the elementary event $\omega$ of twins and we define the possible results of the elemetary event as Monozygotic twins = M Dizygotic twins = D

The event space $\Omega = (M, D)$

Hence the probabilities for the two possible outcomes given there are twins is $P(T | \Omega)=1/4, P(DT| \Omega)=3/4$ So that's the wording and symbol for a conditional probability. Now we look on the second event of Twins have the identical sex ($I$) or not (the C stands for complementary, hence for not having the same sex) ($I^C$). Given the twins are monozygotic, the probability is $P(I | M)=1$ and $P(I^C| M)=0$ they are both either female or male, for the dizygotical twins it is: $P(I | M)=P(I^C| D)=1/2$

Let as look on the situation with a probability tree:

![Tree_digramm of monozygotic and dizygcotic twins](Tree_diagram.png)

There are only a view rules to be recoginzed we know from the set theory :\
1. Probabilities on a branches are multiplied\
2. The sum of all probabilities of the different branches has to be 1.\
3. The commutative law of intersect of sets is for the branches with for the monozygotic twins have the same sex :

$D\ \cap\ I\ =\ I\ \cap\ D$

And the same is true for probabilities, we have only to go backwards on the brach:

$P(M)*P\left(I|M\right)=P\left(M|I\right)*P(I)$

Now we can rearrange this formula and get the Bayesian Theorem:

$P\left(M|I\right)=\frac{P(M)*P\left(I|M\right)}{P(I)}$

Hence e.g. we can answer the question what is the probability for monozygcotic twins given the babies have the same sex, if we know the terms on the right site of the formula. This is because it is often called inverse probability..

The only term we do not know yet is $P(I)$. This information we can get out of the tree diagram too. It is in principle a new smaller event space which lead to higher probabilities for the elementary events in this space. The space is identified with the frame:

![Tree_digramm of monozygotic and dizygcotic twins with identical sex](Tree_frame_diagram.png)

Now we can summarize over the branches containing the state I:

$P(I)=\ P(M)*P\left(I|M\right)+P(D)*P\left(I|D\right)$

The formula looks quite strait forward, but the term $P(I)$ called marginal Likelihood especially is usually a monster as the name itself. To summarize over all possible state of the new "event sapace" in more complex investigations is not easy and especially in the case of continuous data integration has to be used instead of summation and numerical Monte Carlo simulation has often to be applies. That was perhaps the reason Bayes Method was not used until the introduction of the computer.

Excerise You have following situation. Two dark boxes containg 10 balls each. In the box A there are 7 red balls and 3 white balls. In the box B there is 1 red ball and 9 white balls. Now the lady lack take out of one box a red ball. What is the probability this ball went out of the box A (inverse probability)? 1.please draw the probability tree and set up the equation to get the inverse probability, and the marginal likelihood:

![Tree_digramm of red and white balls in two boxes](Balls_in_boxes.png)

$P(A)*P\left(R|A\right)=P\left(A|R\right)*P(R)$

The inverse probability the red ball went out of box A is $P\left(A|R\right)=\frac{P(A)*P\left(R|A\right)}{P(R)}$

The marginal likelihood is given as $P(R)=\ P(A)*P\left(R|A\right)+P(B)*P\left(R|A\right)$

2.  What is the marginal Likelihood in simple words? This is nothing else then the probability to take a red ball out all the boxes. It is the probability of the subset red balls in the elementary space "balls in the boxes", there are 8 red balls within 20 balls. To draw a red ball has the probability of 8/20= 2/5. For this reason Hence P(R)

$P(R)=\frac{1}{2}*\frac{7}{10}+\frac{1}{2}*\frac{1}{10}=\frac{8}{20}$

Unfortunately it is called as a Likelihood, but it is not, in fact the marginal Likelihood is a probability! Likelihood looks how good a parameter can explain the data, it is not a probability but a way to find the maximum likelihood for the data given a distribution. In the case of Gaussian distribution e.g. the maximum likelihood is the mean value. This is where confusion starts, with imprecise wording.

4.Eventually get the probability we looking for. $P(A|R)=\frac{\frac{7}{20}}{\frac{7}{20}+\frac{1}{20}}=\frac{7}{8}$

And here it is obvious. The elementary space is reduced the elementary case red balls in box A (probability 7/8) and red box in box B (probability 1/8).

What is the probability to draw a red ball out of the box A = 1/2\*7/10 = 7/20. This probability is much lower than that one if you know a red ball is taken so where it is coming form.There is where the intuitive is ends.

2.Show the "marginal likelihood" and determine it. 3.what is "marginal likelihood" all about?

How the information: It is a red ball" change the probability in contrast to the experiment "drawing a red ball out of the boxes?

5.Which other random experiment resembles it now? Can you express it?

Hier ist eine Inline-Formel: $P\left(p\right)=1\ for\ p\ \in[0,1]$.

-\> Apply on a clinical data like what is the probability the test is positive the patient is ill or decision making as in differential diagnoses

Up to now we have examples where we do not speak about probability distribution. The probabilities were obvious by counting or given by information Now we go to examples were the probability has to be calculated out o a distribution because counting is not possible or very cumbersome.

## Laplace´ data of births in Paris

Let us come back to the Laplace experiment with the birth rate of girls and boys in Paris from 1745 to 1770? The elementary events with the results "yes" or no "are" binomial distributed, and can be applied for the birth rates. What Laplace wanted like to show was the possibility for a birth rate of 0.5 when the number of the girls born in the observed period is 241945 with total birth of 493472.

We start with the probability of finding $k_G$ female births with a birth rate of (probability of the elementary event of a birth) 0.5. The possible number of births are binomial distributed hence we get for the probability $P(k_G|p,t)$ following:

$P(k_G|\ p,\ t)=\left(\begin{matrix}t\\k_G\\\end{matrix}\right)p^{k_G}{(1-p)}^{t-k_G}$

To get the formula of the posteriori probability we have to change the parameter p and $k_G$ to get the probability in question:

$P(p|\ k_G,\ t)=\frac{P(k_G|\ p,\ t)P(p)}{P(k_G, \ t)}$

Where:

$P(p|\ k_G,\ t)$ : The posterior probability of p given the data $k_G$ and total trials t.

$P(k_G|\ p,\ t)$: The probability of k births of girls given the parameter p

$P(p)$: The probability (density) of p. With Laplace´ a priori assumption in the discrete case we handle P(p=0.5)=1

$P(k_G|\ t)$: This is marginal likelihood.

We have given following information:

1.  $P(k_G|p,t)$ given via the formula of the binomila distribution. With the binomial coefficient

$\left(\begin{matrix}t\\k_G\\\end{matrix}\right)=\frac{t!}{k_G!(t-k_G)!}$

If we intrduce the numerical value we can see that the numbers to be calculated are much to small as to be handled by hand or the computer

$P(k_G=241945|\ p=0.5,\ t=493472)=\left(\begin{matrix}493472\\241945\\\end{matrix}\right){0.5}^{241945}{(1-0.5)}^{493472-241945}$

What we can do is to build the logarithm, all number are getting much smaller and multiplication is now changed to addition and the power operation to a multiplication:

$ln\left[P(k_G|\ p,\ t)\right]=ln\left(\begin{matrix}t\\k_G\\\end{matrix}\right)+(k_G)ln(p)+(t-k_G)ln(1-p)$

Can be obtained by the very good Stirling approximations for high values of :

$ln\left(\begin{matrix}t\\k_G\\\end{matrix}\right)\approx t*ln(t)-k_Gln(k_G)-(t-k_G)ln(t-k)-\frac{(1-p)}{2}ln(2\pi t p)$

2.  The a priori probability is the parameter $P(p=0.5)$. Laplace set the probability to 1 . That´s the "knowledge" before he may get a new value out of his investigation. $P(p=0.5)$ = 1
3.  And what is the value $P(k_G| \ t)$ ? This is often the elephant in the room and the promised monster:

$P(k_G| t)=\int_{0}^{1}{P(k_G|p,t)*P(p)dp}$

It is the integration (you can call it a summation for the continuous variable p) over all possible values of the elementary probability P(p) for the parameter p weighted by its probability ... . But we should remember it is the probability of the subset of the count of births $k_G$ for each parameter p between the interval \[0,1\] in the whole elementary space (all $k_G$ in the interval \[0, 493472\]?)

In the most cases this integral cannot be determined or it is very cumbersome. But each integral can be seen as summation over infinite increments. This is the basic of each approximation of an integral and that´s how a Monte Carlo process it is working. E.g. we

$P(p|k_G,t)\approx\frac{1}{m}\sum_{i=0}^{m}{P(k|p_i, t)}P(p_i)$

with the increment $p_i=\frac{i}{m}$

In the case of m = 10 we get $p_i$ =0.1, 0.2, ....0.5,... 0.9, and we assume for all the same probability, hence each it has the probability $P( p_i )$=1.

Furthermore we have again the binomial distribution

$P(k_G|\ p_i,\ t)=\left(\begin{matrix}t\\k_G\\\end{matrix}\right)p_i^{k_G}{(1-p_i)}^{t-k_G}$

We get finally the approximation:

$P(p|k_G,t)\approx\frac{1}{m}\sum_{i=0}^{m}\left(\begin{matrix}t\\k_G\\\end{matrix}\right)p_i^{k_G}{(1-p_i)}^{t-k_G}$

The binomial coefficient is the same as given above. The $p_i$ can be run over different values and summed up.

So this is quite easy to compute. It can be done with different values of i to look on the distribution of that approximation. One thing has to be taken into account. The term

## R-Code for $P(k_G|p,t)$

```{r}
fbirths_vect <- c(0.51, 241945, 493472) #parameter vector (fbirths_vect) of the female births 

ln_bino_coef <- fbirths_vect[3]*log(fbirths_vect[3])-
                fbirths_vect[2]*log(fbirths_vect[2])-
              (fbirths_vect[3]-fbirths_vect[2])*log(fbirths_vect[3]-fbirths_vect[2])-
               log(2*pi*fbirths_vect[3]*fbirths_vect[1]*(1-fbirths_vect[1]))/2

ln_coef <- fbirths_vect[2]*log(fbirths_vect[1])+
            (fbirths_vect[3]-fbirths_vect[2])*log(1-fbirths_vect[1])

ln_bino_dist <- ln_bino_coef+ln_coef


bino_dist<-exp(ln_bino_dist)


cat("The probability of k births of girls given the parameter p",fbirths_vect[1],":", bino_dist, "\n")

```

## R-Code for $P(k_G|t)$

according to the approximation in 3.

```{r}
# values for fbirths_vect[2] and fbirths_vect[3]
fbirths_vect_2 <- fbirths_vect[2]
fbirths_vect_3 <- fbirths_vect[3]

# range for fbirths_vect[1]
start_value <- 0.488    # start value
end_value <- 0.509      # end value
step_size <- 0.001      # Ssteps

# Generation of vector fbirths_vect[1]
fbirths_vect_1 <- seq(from = start_value, to = end_value, by = step_size)

# Empty list for the results
results <- list()

# Varaible with start value of the summed results
total_bino_dist <- 0

# Loop über die Werte von fbirths_vect[1]
for (value in fbirths_vect_1) {
  ln_bino_coef <- fbirths_vect_3 * log(fbirths_vect_3) -
                  fbirths_vect_2 * log(fbirths_vect_2) -
                  (fbirths_vect_3 - fbirths_vect_2) * log(fbirths_vect_3 - fbirths_vect_2) -
                  log(2 * pi * fbirths_vect_3 * value * (1 - value)) / 2

  ln_coef <- fbirths_vect_2 * log(value) +
             (fbirths_vect_3 - fbirths_vect_2) * log(1 - value)
  
  ln_bino_dist <- ln_bino_coef + ln_coef
  
  # save the results in list
  results[[as.character(value)]] <- exp(ln_bino_dist)
  
  # Sum up the values in the list
  total_bino_dist <- total_bino_dist + exp(ln_bino_dist)
}

# Display the results
print(results)


cat("The approximation of the marginal likelihood:", total_bino_dist, "\n")

```

## R-code for the final result of the a posteriori probability

for the parameter p=0.5 given $k_G=241945$

```{r}

cat("a posteriori probability for the parameter",fbirths_vect[1],":",bino_dist/total_bino_dist )
```

But this is not still end of story. He only know the paramter 0.5 given the data is very very unlikely. But he knew $P(p<0.5)\approx\ 1-P(p>0.5)$ (because $P(p=0.5 \approx\  0)$ and all higher probabilites of p get even samller he conclude $P(p<0.5)\approx\ 1$. 
This is what Laplace get as the result. He publised it 1820 and he was as ‘certain as any other moral truth’ that
boys were born more frequently than girls.

Excersice

Please use the Z-test to look if the Nullypothese p=0.5 can be rejected 


